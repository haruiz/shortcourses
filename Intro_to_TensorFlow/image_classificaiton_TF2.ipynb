{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T1Pf4mbuVRI"
   },
   "source": [
    "\n",
    "#Image Classification From Scratch  With TensorFlow 2.0\n",
    "\n",
    "In this notebook, we'll be training a Convolutional Neural Network ( CNN ) without using Keras, all with raw TF 2.0 APIs!\n",
    "\n",
    "We'll declare weights, loss function and a optimizer for our CNN and train it on the Horses-Or-Humans dataset available on TensorFlow Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcqgU2fXu-Yw"
   },
   "source": [
    "\n",
    "## 1) Installing TensorFlow 2.0\n",
    "\n",
    "First, if our Colab's default TensorFlow version is not 2.0 then we need to upgrade the TF package using `pip`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkcq3St-gZsd"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOhEmurMvlO9"
   },
   "source": [
    "\n",
    "## 2) Importing packages and declaring useful variables\n",
    "\n",
    "We declare some useful variables like `batch_size`, `padding` for convolutional layers, the `learning_rate` and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYda818kgpz8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "padding = \"SAME\"  #@param ['SAME', 'VALID' ]\n",
    "num_output_classes = 102  #@param {type: \"number\"}\n",
    "batch_size = 32  #@param {type: \"number\"}\n",
    "learning_rate = 0.001  #@param {type: \"number\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjsIAAxzxoii"
   },
   "source": [
    "\n",
    "## 3) Fetching the Dataset\n",
    "\n",
    "We fetch our dataset from TensorFlow Datasets which makes much of the data preprocessing easier ;-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XglEBv0KjNF0"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = 'horses_or_humans'  #@param {type: \"string\"}\n",
    "\n",
    "dataset = tfds.load( name=dataset_name , split=tfds.Split.TRAIN )\n",
    "dataset = dataset.shuffle( 1024 ).batch( batch_size )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sZ3W9NCx8RB"
   },
   "source": [
    "\n",
    "## 4) Defining CNN operations\n",
    "\n",
    "For our CNN model, we mainly use three operations ( layers ).\n",
    "\n",
    "1. `conv2d` : Performs convolutions over the `inputs` matrix with kernels ( `filters` ) and `stride_size`. Also, it performs the Leaky ReLU activation function.\n",
    "\n",
    "2. `maxpool` : Performs max pooling over the `inputs`.\n",
    "\n",
    "3. `dense` : Dense layers for the CNN with dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUwYW2IXg8hl"
   },
   "outputs": [],
   "source": [
    "\n",
    "leaky_relu_alpha = 0.2 #@param {type: \"number\"}\n",
    "dropout_rate = 0.5 #@param {type: \"number\"}\n",
    "\n",
    "def conv2d( inputs , filters , stride_size ):\n",
    "    out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding ) \n",
    "    return tf.nn.leaky_relu( out , alpha=leaky_relu_alpha ) \n",
    "\n",
    "def maxpool( inputs , pool_size , stride_size ):\n",
    "    return tf.nn.max_pool2d( inputs , ksize=[ 1 , pool_size , pool_size , 1 ] , padding='VALID' , strides=[ 1 , stride_size , stride_size , 1 ] )\n",
    "\n",
    "def dense( inputs , weights ):\n",
    "    x = tf.nn.leaky_relu( tf.matmul( inputs , weights ) , alpha=leaky_relu_alpha )\n",
    "    return tf.nn.dropout( x , rate=dropout_rate )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XNdAsWky7eg"
   },
   "source": [
    "\n",
    "## 5) Initializing CNN weights\n",
    "\n",
    "We initialize the weights for our CNN. The shapes need to calculated but the `tf.nn.conv2d` expects the filters to have a shape of `[ kernel_size , kernel_size , in_dims , out_dims ]`.\n",
    "\n",
    "We use the `glorot_uniform` initializer for our weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXSI7DnehJhE"
   },
   "outputs": [],
   "source": [
    "\n",
    "output_classes = 3\n",
    "initializer = tf.initializers.glorot_uniform()\n",
    "def get_weight( shape , name ):\n",
    "    return tf.Variable( initializer( shape ) , name=name , trainable=True , dtype=tf.float32 )\n",
    "\n",
    "shapes = [\n",
    "    [ 3 , 3 , 3 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 32 ] , \n",
    "    [ 3 , 3 , 32 , 32 ] ,\n",
    "    [ 3 , 3 , 32 , 64 ] , \n",
    "    [ 3 , 3 , 64 , 64 ] ,\n",
    "    [ 3 , 3 , 64 , 128 ] , \n",
    "    [ 3 , 3 , 128 , 128 ] ,\n",
    "    [ 3 , 3 , 128 , 256 ] , \n",
    "    [ 3 , 3 , 256 , 256 ] ,\n",
    "    [ 3 , 3 , 256 , 512 ] , \n",
    "    [ 3 , 3 , 512 , 512 ] ,\n",
    "    [ 8192 , 3600 ] , \n",
    "    [ 3600 , 2400 ] ,\n",
    "    [ 2400 , 1600 ] , \n",
    "    [ 1600 , 800 ] ,\n",
    "    [ 800 , 64 ] ,\n",
    "    [ 64 , output_classes ] ,\n",
    "]\n",
    "\n",
    "weights = []\n",
    "for i in range( len( shapes ) ):\n",
    "    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i ) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsCz6ONvziKX"
   },
   "source": [
    "\n",
    "## 6) Assembling the operations\n",
    "\n",
    "We put together all the CNN ops we defined earlier into a final model which we will use for training finally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Py8Ggzs2hPqO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def model( x ) :\n",
    "    x = tf.cast( x , dtype=tf.float32 )\n",
    "    c1 = conv2d( x , weights[ 0 ] , stride_size=1 ) \n",
    "    c1 = conv2d( c1 , weights[ 1 ] , stride_size=1 ) \n",
    "    p1 = maxpool( c1 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c2 = conv2d( p1 , weights[ 2 ] , stride_size=1 )\n",
    "    c2 = conv2d( c2 , weights[ 3 ] , stride_size=1 ) \n",
    "    p2 = maxpool( c2 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c3 = conv2d( p2 , weights[ 4 ] , stride_size=1 ) \n",
    "    c3 = conv2d( c3 , weights[ 5 ] , stride_size=1 ) \n",
    "    p3 = maxpool( c3 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c4 = conv2d( p3 , weights[ 6 ] , stride_size=1 )\n",
    "    c4 = conv2d( c4 , weights[ 7 ] , stride_size=1 )\n",
    "    p4 = maxpool( c4 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c5 = conv2d( p4 , weights[ 8 ] , stride_size=1 )\n",
    "    c5 = conv2d( c5 , weights[ 9 ] , stride_size=1 )\n",
    "    p5 = maxpool( c5 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c6 = conv2d( p5 , weights[ 10 ] , stride_size=1 )\n",
    "    c6 = conv2d( c6 , weights[ 11 ] , stride_size=1 )\n",
    "    p6 = maxpool( c6 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    flatten = tf.reshape( p6 , shape=( tf.shape( p6 )[0] , -1 ))\n",
    "\n",
    "    d1 = dense( flatten , weights[ 12 ] )\n",
    "    d2 = dense( d1 , weights[ 13 ] )\n",
    "    d3 = dense( d2 , weights[ 14 ] )\n",
    "    d4 = dense( d3 , weights[ 15 ] )\n",
    "    d5 = dense( d4 , weights[ 16 ] )\n",
    "    logits = tf.matmul( d5 , weights[ 17 ] )\n",
    "\n",
    "    return tf.nn.softmax( logits )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_P-L8iQWz29J"
   },
   "source": [
    "\n",
    "## 7) Defining the loss function and optimization using `tf.GradientTape`\n",
    "\n",
    "We use `tf.losses.categorical_crossentropy` as our loss function. Now comes `tf.GradientTape`, an automatic differentiation engine which records all the derivatives within its scope.\n",
    "\n",
    "The `train_step` function takes in a batch of data, calculates the loss and gradients and then with `optimizer.apply_gradients`, we update the `weights`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U61C5trri_xu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss( pred , target ):\n",
    "    return tf.losses.categorical_crossentropy( target , pred )\n",
    "\n",
    "optimizer = tf.optimizers.Adam( learning_rate )\n",
    "\n",
    "def train_step( model, inputs , outputs ):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss( model( inputs ), outputs)\n",
    "    grads = tape.gradient( current_loss , weights )\n",
    "    optimizer.apply_gradients( zip( grads , weights ) )\n",
    "    print( tf.reduce_mean( current_loss ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8CUgMw70V9P"
   },
   "source": [
    "\n",
    "## 8) Final Training\n",
    "\n",
    "We train our model for a specific number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DL42kBKgk2U2"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 256 #@param {type: \"number\"}\n",
    "\n",
    "for e in range( num_epochs ):\n",
    "    for features in dataset:\n",
    "        image , label = features[ 'image' ] , features[ 'label' ]\n",
    "        train_step( model , image , tf.one_hot( label , depth=3 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSdM0ti0l-lO"
   },
   "source": [
    "\n",
    "That's All. You can now export the graph or play around with more layers or fine-tuning the hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Image_Classification_TF2.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "15QZMSIrvE4MgVq2GnQ0XUsqQssY6sn7w",
     "timestamp": 1572616939723
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
